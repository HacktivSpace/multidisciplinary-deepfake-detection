{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "This notebook evaluates the performance of different models trained for the Multidisciplinary Deepfake Detection product. It includes steps for loading trained models, making predictions, calculating performance metrics, and visualizing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "import torch\n",
    "\n",
    "# To set up logging\n",
    "import logging\n",
    "logging.basicConfig(filename='../logs/model_evaluation.log', level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s: %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# To load configuration\n",
    "from src.config import Config\n",
    "from src.dataset.data_loader import load_csv_data\n",
    "from src.utils.metrics import calculate_metrics, plot_confusion_matrix\n",
    "\n",
    "logging.info(\"Model evaluation started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To load Processed Data\n",
    "\n",
    "To load the processed data for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load processed data\n",
    "processed_data_path = os.path.join(Config.PROCESSED_DATA_DIR, 'processed_data.csv')\n",
    "logging.info(\"Loading processed data from {}.\".format(processed_data_path))\n",
    "data = load_csv_data(processed_data_path)\n",
    "X_test = data.drop('label', axis=1)\n",
    "y_test = data['label']\n",
    "logging.info(\"Processed data loaded successfully with shape {}.\".format(data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To evaluate CNN Model\n",
    "\n",
    "To load and evaluate the CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To evaluate CNN model\n",
    "logging.info(\"Evaluating CNN model...\")\n",
    "cnn_model_path = os.path.join(Config.MODEL_DIR, 'cnn_model.h5')\n",
    "cnn_model = load_model(cnn_model_path)\n",
    "logging.info(\"CNN model loaded from {}.\".format(cnn_model_path))\n",
    "\n",
    "# To make predictions\n",
    "y_pred_cnn = (cnn_model.predict(X_test) > 0.5).astype('int32')\n",
    "\n",
    "# To calculate metrics\n",
    "metrics_cnn = calculate_metrics(y_test, y_pred_cnn)\n",
    "logging.info(\"CNN Model Metrics: {}\".format(metrics_cnn))\n",
    "print(\"CNN Model Metrics:\\n\", metrics_cnn)\n",
    "\n",
    "# To plot confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred_cnn, labels=[0, 1], output_dir='../logs', filename='cnn_confusion_matrix.png')\n",
    "logging.info(\"CNN model evaluation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To evaluate Transformer Model\n",
    "\n",
    "To load and evaluate the Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To evaluate Transformer model\n",
    "logging.info(\"Evaluating Transformer model...\")\n",
    "transformer_model_path = os.path.join(Config.MODEL_DIR, 'transformer_model.pth')\n",
    "transformer_model = torch.load(transformer_model_path)\n",
    "transformer_model.eval()\n",
    "logging.info(\"Transformer model loaded from {}.\".format(transformer_model_path))\n",
    "\n",
    "# To convert test data to tensor\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    y_pred_transformer = transformer_model(X_test_tensor).numpy()\n",
    "y_pred_transformer = (y_pred_transformer > 0.5).astype('int32')\n",
    "\n",
    "# To calculate metrics\n",
    "metrics_transformer = calculate_metrics(y_test, y_pred_transformer)\n",
    "logging.info(\"Transformer Model Metrics: {}\".format(metrics_transformer))\n",
    "print(\"Transformer Model Metrics:\\n\", metrics_transformer)\n",
    "\n",
    "# To plot confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred_transformer, labels=[0, 1], output_dir='../logs', filename='transformer_confusion_matrix.png')\n",
    "logging.info(\"Transformer model evaluation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To evaluate SVM Model\n",
    "\n",
    "To load and evaluate the SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To evaluate SVM model\n",
    "logging.info(\"Evaluating SVM model...\")\n",
    "svm_model_path = os.path.join(Config.MODEL_DIR, 'svm_model.pkl')\n",
    "svm_model = joblib.load(svm_model_path)\n",
    "logging.info(\"SVM model loaded from {}.\".format(svm_model_path))\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "# To calculate metrics\n",
    "metrics_svm = calculate_metrics(y_test, y_pred_svm)\n",
    "logging.info(\"SVM Model Metrics: {}\".format(metrics_svm))\n",
    "print(\"SVM Model Metrics:\\n\", metrics_svm)\n",
    "\n",
    "# To plot confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred_svm, labels=[0, 1], output_dir='../logs', filename='svm_confusion_matrix.png')\n",
    "logging.info(\"SVM model evaluation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Bayesian Model\n",
    "\n",
    "To load and evaluate the Bayesian model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To evaluate Bayesian model\n",
    "logging.info(\"Evaluating Bayesian model...\")\n",
    "bayesian_model_path = os.path.join(Config.MODEL_DIR, 'bayesian_model.pkl')\n",
    "bayesian_model = joblib.load(bayesian_model_path)\n",
    "logging.info(\"Bayesian model loaded from {}.\".format(bayesian_model_path))\n",
    "\n",
    "# Make predictions\n",
    "y_pred_bayesian = bayesian_model.predict(X_test)\n",
    "y_pred_bayesian = (y_pred_bayesian > 0.5).astype('int32')\n",
    "\n",
    "# To calculate metrics\n",
    "metrics_bayesian = calculate_metrics(y_test, y_pred_bayesian)\n",
    "logging.info(\"Bayesian Model Metrics: {}\".format(metrics_bayesian))\n",
    "print(\"Bayesian Model Metrics:\\n\", metrics_bayesian)\n",
    "\n",
    "# To plot confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred_bayesian, labels=[0, 1], output_dir='../logs', filename='bayesian_confusion_matrix.png')\n",
    "logging.info(\"Bayesian model evaluation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Vision Transformer Model\n",
    "\n",
    "To load and evaluate the Vision Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To evaluate Vision Transformer model\n",
    "logging.info(\"Evaluating Vision Transformer model...\")\n",
    "vision_transformer_model_path = os.path.join(Config.MODEL_DIR, 'vision_transformer_model.pth')\n",
    "vision_transformer_model = torch.load(vision_transformer_model_path)\n",
    "vision_transformer_model.eval()\n",
    "logging.info(\"Vision Transformer model loaded from {}.\".format(vision_transformer_model_path))\n",
    "\n",
    "# To convert test data to tensor\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    y_pred_vt = vision_transformer_model(X_test_tensor).numpy()\n",
    "y_pred_vt = (y_pred_vt > 0.5).astype('int32')\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_vt = calculate_metrics(y_test, y_pred_vt)\n",
    "logging.info(\"Vision Transformer Model Metrics: {}\".format(metrics_vt))\n",
    "print(\"Vision Transformer Model Metrics:\\n\", metrics_vt)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred_vt, labels=[0, 1], output_dir='../logs', filename='vision_transformer_confusion_matrix.png')\n",
    "logging.info(\"Vision Transformer model evaluation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The model evaluation has been completed for all trained models. The performance metrics and confusion matrices provide insights into the strengths and weaknesses of each model, guiding further improvements and fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Model evaluation completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
