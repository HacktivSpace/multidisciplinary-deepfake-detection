{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This notebook handles the preprocessing of raw data for the Multidisciplinary Deepfake Detection product. It includes steps for loading the raw data, cleaning it, encoding categorical features, normalizing numerical features, and splitting the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# To set up logging\n",
    "logging.basicConfig(filename='../logs/data_preprocessing.log', level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s: %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# To load configuration\n",
    "from src.config import Config\n",
    "\n",
    "# To define paths\n",
    "raw_data_path = os.path.join(Config.RAW_DATA_DIR, 'sample_data.csv')\n",
    "processed_data_path = os.path.join(Config.PROCESSED_DATA_DIR, 'processed_data.csv')\n",
    "\n",
    "logging.info(\"Data preprocessing started.\")\n",
    "\n",
    "# To load raw data\n",
    "logging.info(\"Loading raw data from {}.\".format(raw_data_path))\n",
    "data = pd.read_csv(raw_data_path)\n",
    "logging.info(\"Raw data loaded successfully with shape {}.\".format(data.shape))\n",
    "\n",
    "# To drop missing values\n",
    "logging.info(\"Dropping missing values.\")\n",
    "data.dropna(inplace=True)\n",
    "logging.info(\"Missing values dropped. Data shape is now {}.\".format(data.shape))\n",
    "\n",
    "# To encode categorical features\n",
    "logging.info(\"Encoding categorical features.\")\n",
    "label_encoders = {}\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "    label_encoders[column] = le\n",
    "logging.info(\"Categorical features encoded successfully.\")\n",
    "\n",
    "# To normalize numerical features\n",
    "logging.info(\"Normalizing numerical features.\")\n",
    "scaler = StandardScaler()\n",
    "numerical_features = data.select_dtypes(include=[np.number]).columns\n",
    "data[numerical_features] = scaler.fit_transform(data[numerical_features])\n",
    "logging.info(\"Numerical features normalized successfully.\")\n",
    "\n",
    "# To split data into training and testing sets\n",
    "logging.info(\"Splitting data into training and testing sets.\")\n",
    "X = data.drop('label', axis=1)\n",
    "y = data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=Config.RANDOM_SEED)\n",
    "logging.info(\"Data split completed. Training data shape: {}, Testing data shape: {}.\".format(X_train.shape, X_test.shape))\n",
    "\n",
    "# To save processed data\n",
    "logging.info(\"Saving processed data to {}.\".format(processed_data_path))\n",
    "processed_data = pd.concat([X_train, y_train], axis=1)\n",
    "processed_data.to_csv(processed_data_path, index=False)\n",
    "logging.info(\"Processed data saved successfully.\")\n",
    "\n",
    "logging.info(\"Data preprocessing completed.\")\n",
    "\n",
    "# To display first few rows of the processed data\n",
    "processed_data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
